
On test set


COREELATIONS
forgot to create page views per visit! do that then check correlations again. 
honestly, could just do something in python to spit out correlations over 0.7


advisits and channel paid search have 93% correlation. will drop adVisits
page views and avgpageviews highly correlated ofc, will keep just the avg
channel grouping social really highly correlated with the source principal components, dropping it
high correlations between is mobil and is desktop. keeping desktop


transactionrev and conversions, not sure. will use rf to determine

0.105 for conversions to future conv
0.091 for transaction rev. keeping conversions



affiliates and istruedirect?



forgot to do this for test set

df1train['transactionRevenue'] = (df1train['transactionRevenue'] / 1000000).round(2)



source_columns_test = [col for col in df2test.columns if col.startswith('source_')]
source_data_test = df2test[source_columns_test]

scaler = StandardScaler()
source_data_scaled_test = scaler.fit_transform(source_data_test)




pca = PCA(n_components=11)
principal_components_test = pca.transform(source_data_scaled_test)

pc_df_test = pd.DataFrame(data=principal_components_test, 
                     columns=[f'PCSource{i+1}' for i in range(11)],
                     index=df2test.index)  # Using the same index to align with the original data



# Concatenating the PCA columns back
df2test = pd.concat([df2test, pc_df], axis=1)


# with this done, the original source columns can be dropped
for col in source_columns_test:
    if col in df2test.columns:
        df2test.drop(col, axis=1, inplace = True)



# dropping unnecessary columns
for col in to_drop:
    if col in df2test.columns:
        df12test.drop(col, axis=1, inplace = True)




from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import numpy as np
import matplotlib.pyplot as plt







On training set 

PCA for source
Dummies then PCA for country
Dummies for continent
Avg page views per visit variable


Replicate for test set, but using optimal components from 



df1train = pd.get_dummies(df1train, columns='country')


then final dummies
Continent



country_data_scaled = scaler.fit_transform(country_data)

pca = PCA(n_components=20)
principal_components = pca.fit_transform(country_data_scaled)
pc_df = pd.DataFrame(data=principal_components, columns=[f'PC{i+1}' for i in range(20)])




**the dummy columns. and there will be some in there which aren't in the orignal. so will need to do new aggregation dictionaries
**the visitor_counts df (same dict)
**the user agg df (newdict)


df2c = pd.get_dummies(df2c, columns=columns_to_encode)


agg2_dict = {
    'country': 'first',
    'continent': 'first',
    'subContinent': 'first',
    'transactionRevenue': 'sum',
    'pageviews': 'sum',
    'isTrueDirect': 'sum'

}

dummy_columns = [col for col in df2c.columns if '_' in col]

for dummy_col in dummy_columns:
    agg_dict2[dummy_col] = 'sum'



# making sure columns match
incorrect_cols = [key for key in agg_dict2 if key not in df2c.columns]
print(incorrect_cols)

useragg2 = df2c.groupby('fullVisitorId').agg(agg2_dict)
useragg.reset_index(inplace=True)

visitor_counts = df2c.groupby('fullVisitorId').agg(aggregations)
visitor_counts.columns = ['visits', 'conversions', 'adVisits']
visitor_counts.reset_index(inplace=True)




**the merges 

df2test = df2test.merge(visitor_counts, on='fullVisitorId', how='left')
df2test= df2test.merge(useragg2, on='fullVisitorId', how='left')





df2 = gstore.loc[gstore['date'] > cutoff_date]




df2p = df2.loc[df2['date'] > df2cutoff, ['date', 'fullVisitorId', 'transactionRevenue']]  # performance
df2c = df2.loc[df2['date'] <= df2cutoff] # collection

for col in to_drop:
    if col in df2c.columns:
        df2c.drop(col, axis=1, inplace = True)


df2c['isTrueDirect'] = df2c['isTrueDirect'].fillna(False).astype(int)
df2c['pageviews'] = df2c['pageviews'].astype(int)



df2trainids = df2c['fullVisitorId'].unique().tolist() 

df2test = pd.DataFrame(df2testids, columns=['fullVisitorId'])


NEXT: label and newvisit thing


Columns which will eventually be needed in train df (pre feature selection or dimensionality reduction)

First pass: Prepare it to be able to do PCA + eng depth with later

** counts or sums (no encoding needed)**
	label for conversion in performance period - DONE
total # of visits (count of times they appear in dataset)
sum of transaction revenue
total # of visits where transaction > 0 (presence of conversions)
pageviews
# of trueDirect visits (can use what i used for label but on df1c)
	presence of a new visit - DONE
visits where adwordsClickInfo.gclId is populated 
visits where campaign is populated

**based on first they appear with**
continent
subcontinent
country


**needs encoding**
channelGrouping 
deviceCategory
Source



Index(['channelGrouping', 'date', 'fullVisitorId', 'sessionId', 'visitId',
       'visitNumber', 'visitStartTime', 'continent', 'subContinent', 'country',
       'region', 'metro', 'city', 'cityId', 'deviceCategory', 'visits', 'hits',
       'pageviews', 'newVisits', 'transactionRevenue', 'campaign', 'source',
       'medium', 'adwordsClickInfo.criteriaParameters', 'isTrueDirect',
       'adwordsClickInfo.page', 'adwordsClickInfo.slot',
       'adwordsClickInfo.gclId', 'adwordsClickInfo.adNetworkType',
       'adwordsClickInfo.isVideoAd', 'adContent'],
      dtype='object')





i want to perform onehotencoding for the following columns [channelGrouping, deviceCategory, source] in df1c, and create columns by feature names for the resulting categories. could you suggest some code?




there are a couple of aggregations i want to do in the dataframe df1c based on fullVisitorId in df1train and then create columns to store the aggregated values for each fullVisotrID in the df1train dataframe. listing them below as well as what i'd like to call the column in parantheses, is there an efficient way to do these? 

sum of transactionRevenue column (spend)
sum of pageviews column (pageViews)
count of total times that fullVisitorID appears in the df (visits)
count of times that fullVistorID has a non-null in the transactionRevenue column (conversions)
count of times where that fullVistorID has a non-null in the adwordsClickInfo.gclId colum (advisits)
count of times where that fullVistorID has a non-null in the campaign colum (campaignvisits)
count of times where that fullVistorID has a non-null in the isTrueDirect column (directvisits)


there were a couple more i wanted to do, listed below. these ones are not aggregations though. would these fit well into the structure you proposed, or is it better to handle these differently

the value in the column continent for the first appearance of that fullVisitorID (continent)
the value in the column subContinent for the first appearance of that fullVisitorID (subcontinent)
the value in the column country for the first appearance of that fullVisitorID (country)



# Step 1: Group by 'fullVisitorId' and perform all the necessary aggregations
aggregations = {
    'transactionRevenue': [
        ('spend', 'sum'),  # Sum of transactionRevenue
        ('conversions', lambda x: x.notnull().sum())  # Count non-null transactionRevenue
    ],
    'pageviews': [
        ('pageViews', 'sum')  # Sum of pageviews
    ],
    'adwordsClickInfo.gclId': [
        ('advisits', lambda x: x.notnull().sum())  # Count non-null adwordsClickInfo.gclId
    ],
    'campaign': [
        ('campaignvisits', lambda x: x.notnull().sum())  # Count non-null campaign
    ],
    'isTrueDirect': [
        ('directvisits', lambda x: x.notnull().sum())  # Count non-null isTrueDirect
    ]
}

# Perform the groupby and aggregate based on the specified aggregations
aggregated_data = df1c.groupby('fullVisitorId').agg(**aggregations)

# Flatten the MultiIndex columns created by the aggregation
aggregated_data.columns = [col[1] for col in aggregated_data.columns]

# Step 2: Count of total visits by fullVisitorId
aggregated_data['visits'] = df1c.groupby('fullVisitorId').size()

# Step 3: Merge the aggregated data back into df1train
df1train = df1train.merge(aggregated_data, on='fullVisitorId', how='left').fillna(0)

# Optionally, convert counts and sums to integers if needed (from float due to NaN handling)
columns_to_int = ['spend', 'pageViews', 'visits', 'conversions', 'advisits', 'campaignvisits', 'directvisits']
df1train[columns_to_int] = df1train[columns_to_int].astype(int)















visits per channel
total # of visits
continent 
subcontinent 
country
visits per device category
page views
page views / visits (engagement depth)
truedirect visits
visits by source
new visit flag (were they a new visitor in the collection period?)
conversion count (did they convert during the collection period
transrevenue (how much did they spend with us in the collection period
count of AdWords sessions 
count of visits per source
count of campaign sessions





SCRAP CODE


# # creating addiitonal columns which do not need encoding
# # Define the aggregation dictionary for all specified metrics and columns
# aggregations = {
#     'transactionRevenue': [
#         ('spend', 'sum'),  # Sum of transactionRevenue
#         ('conversions', lambda x: x.notnull().sum())  # Count non-null transactionRevenue
#     ],
#     'pageviews': [
#         ('pageViews', 'sum')  # Sum of pageviews
#     ],
#     'adwordsClickInfo.gclId': [
#         ('advisits', lambda x: x.notnull().sum())  # Count non-null adwordsClickInfo.gclId
#     ],
#     'campaign': [
#         ('campaignvisits', lambda x: x.notnull().sum())  # Count non-null campaign
#     ],
#     'isTrueDirect': [
#         ('directvisits', lambda x: x.notnull().sum())  # Count non-null isTrueDirect
#     ],
#     'continent': [
#         ('continent', 'first')  # Value of continent at the first appearance
#     ],
#     'subContinent': [
#         ('subcontinent', 'first')  # Value of subContinent at the first appearance
#     ],
#     'country': [
#         ('country', 'first')  # Value of country at the first appearance
#     ]
# }




# Perform the groupby and aggregate based on the specified aggregations
aggregated_data = df1c.groupby('fullVisitorId').agg(**aggregations)

# Flatten the MultiIndex columns created by the aggregation
aggregated_data.columns = [col[1] for col in aggregated_data.columns]

# Count of total visits by fullVisitorId
aggregated_data['visits'] = df1c.groupby('fullVisitorId').size()



# Merge the aggregated data back into df1train
df1train = df1train.merge(aggregated_data, on='fullVisitorId', how='left').fillna(0)

# Optionally, convert counts and sums to integers if needed (from float due to NaN handling)
columns_to_int = ['spend', 'pageViews', 'visits', 'conversions', 'advisits', 'campaignvisits', 'directvisits']
df1train[columns_to_int] = df1train[columns_to_int].astype(int)






## THIS WORKED. 
# Step 1: Create the OneHotEncoder instance
encoder = OneHotEncoder(sparse=False)

# Step 2: Fit and transform the data
channel_encoded = encoder.fit_transform(df1c[['channelGrouping']])

# Step 3: Convert the numpy array back to a DataFrame
channel_encoded_df = pd.DataFrame(channel_encoded, columns=encoder.get_feature_names_out(['channelGrouping']))

# Step 4: Concatenate the new dataframe with the original dataframe minus the original 'channelGrouping' column
df1c = pd.concat([df1c.drop('channelGrouping', axis=1), channel_encoded_df], axis=1)


cg = ['channelGrouping_(Other)', 'channelGrouping_Affiliates',
       'channelGrouping_Direct', 'channelGrouping_Display',
       'channelGrouping_Organic Search', 'channelGrouping_Paid Search',
       'channelGrouping_Referral', 'channelGrouping_Social']


# Step 1: Group by 'fullVisitorId' in df1c and sum the columns in cg
grouped = df1c.groupby('fullVisitorId')[cg].sum()

# Step 2: Merge this grouped data back into df1train
df1train = df1train.merge(grouped, on='fullVisitorId', how='left')



# Step 1: Count occurrences of each 'fullVisitorId' in df1c
visitor_counts = df1c['fullVisitorId'].value_counts().reset_index()
visitor_counts.columns = ['fullVisitorId', 'totalVisits']

# Step 2: Merge this count DataFrame back into df1train
df1train = df1train.merge(visitor_counts, on='fullVisitorId', how='left')


# continue later
# df1c.to_csv('df1c.csv', index=True)
# df1train.to_csv('df1train.csv', index=True)

